Vector space
============

.. role:: latex(raw)
     :format: latex

Definitions
-----------

Let :latex:`$(F, +, \times)$` be a field.

.. raw:: latex

    \begin{definition}[Vector space]
      A $F$-vector space $(V, +, .)$ is defined by:
      \begin{itemize}
        \item $V$ is a set, which items are called vectors
        \item $+$ is an operator $(V \times V) \rightarrow V$ (the vector addition)
        \item $.$ is an operator $(F \times V) \rightarrow V$ (the scalar multiplication)
        \item $(V, +)$ is a commutative group, its neutral element is named $0_V$
        \item $\forall \lambda \in F, \forall x, y \in V, \lambda . x + y \in V$
        \item $\forall \lambda, \mu \in F, \forall x \in V, (\lambda \mu) . x = \lambda . (\mu . x)$
              (the scalar multiplication and the field multiplication are compatible)
        \item $\forall x \in V, 1_F . x = x$
        \item $\forall \lambda, \mu \in F, \forall x \in V, (\lambda + \mu) . x = (\lambda . x) + (\mu . x)$
              (distributivity)
        \item $\forall \lambda \in F, \forall x, y \in V, \lambda . (x + y) = (\lambda . x) + (\lambda . y)$
              (distributivity)
      \end{itemize}
    \end{definition}

Properties:

* :latex:`$\forall \lambda \in F, \forall x \in V, -(\lambda . x) = (-\lambda) . x$`
* :latex:`$\forall x \in V, 0_F . x = 0_V$`
* :latex:`$\forall \lambda \in F, \lambda . 0_V = 0_V$`
* :latex:`$\forall \lambda \in F, \forall x \in V, \lambda . x = 0_V \Leftrightarrow \lambda = 0_F \lor x = 0_V$`

.. raw:: latex

    \begin{definition}[Almost empty sequence]
      A sequence $(u_n)_{n \in \N} \in F^\N$ is almost empty when there exists a rank from which every item is null:
      \begin{displaymath}
        \exists n_0, \forall n \ge n_0, u_n = 0_K
      \end{displaymath}
    \end{definition}

An almost empty sequence allows writing infinite sums and products across :latex:`$\N$` which are in fact finite.
Among other things, it allows writing the following definition.

.. raw:: latex

    \begin{definition}[Linear combination]
      Given an empty sequence of coefficients $(\lambda_n) \in F^\N$ and an almost empty sequence of vectors $(x_n) \in V^\N$,
      the linear combination of these items is defined as the finite sum:
      \begin{displaymath}
        \sum_{n \in \N} \lambda_n . x_n \in V
      \end{displaymath}
    \end{definition}

    \begin{definition}[Trivial and non-trivial linear combination]
      A linear combination $\sum_{n \in \N} \lambda_n . x_n$ is said to be trivial is every $\lambda_n$ is null.
      It is said to be non-trivial if at least one $\lambda_n$ is not null.
    \end{definition}

    \begin{definition}[Linear span]
      The linear span of a set of vectors $S = (x_k) \in V^\N$ is the set of linear combinations computed from these vectors:
      \begin{displaymath}
        \text{span}(S) = \left\{\sum_{k \in \N} \lambda_k . x_k : (\lambda_k)_{k \in \N} \text{ almost empty sequence of } F^\N\right\}
      \end{displaymath}
    \end{definition}

    \begin{definition}[Linear span of a finite set]
      The linear span of a set of vectors $x_0, x_1, ..., x_n \in V$ is the set of linear combinations computed from these vectors:
      \begin{displaymath}
        \text{span}(x_0, x_1, ..., x_n) = \left\{\sum_{k \in \llbracket 0, n\rrbracket} \lambda_k . x_k : (\lambda_k)_{k \in \llbracket 0, n\rrbracket} \in F^n\right\}
      \end{displaymath}
    \end{definition}

    \begin{theorem}[Linear span]
      The linear span of a set of vectors $S \in V^\N$ is a $F$-vector space.
      This vector space is called the vector subspace of $V$ generated by $S$.
      Moreover the neutral item, $0_V$, belongs to this vector subspace.
    \end{theorem}

    \begin{definition}[Linearly dependent vectors]
      The vectors $x_0, x_1, ..., x_n \in V$ are linearly dependent if there exists scalars $\lambda_0, \lambda_1, ..., \lambda_n \in K$ not all zero such that:
      \begin{displaymath}
        \sum_{k \in \llbracket 0, n\rrbracket} \lambda_k . x_k = 0_V
      \end{displaymath}
      Then one vector can be written as a linear combination of the other ones.
    \end{definition}

    \begin{definition}[Linearly independent vectors]
      The vectors $x_0, x_1..., x_n \in V$ are linearly independent if:
      \begin{displaymath}
        \forall (\lambda_k)_{k \in \llbracket 0, n\rrbracket} \in F^n, \sum_{k \in \llbracket 0, n\rrbracket} \lambda_k . x_k = 0_V
        \Leftrightarrow \lambda_0 = \lambda_1 = ... = \lambda_n = 0_F
      \end{displaymath}
    \end{definition}

These last definitions can also be written with infinite sets of vectors, using almost empty scalar sequences.

.. raw:: latex

    \begin{definition}[Generators]
      A set of vectors $S \in V^\N$ is said to generate $V$ when $V = \text{span}(S)$.
      In such a case, the items of $S$ are generators of $V$.
    \end{definition}

    \begin{definition}[Basis]
      A set of vectors $S \in V^\N$ is a basis of $V$ when it generates $V$ and contains linearly independent vectors.
    \end{definition}

    \begin{theorem}[Non-trivial coordinates]
      If $\{b_0, b_1, ..., b_{n-1}\}$ is a basis of $V$, every vector $x \in V$ can be written as a linear combination of $b_i$:
      \begin{displaymath}
        \exists (x_i) \in F^n, x = \sum_{i=0}^{n-1} x_i b_i
      \end{displaymath}
      This list $(x_i)$ is unique and this linear combination is non-trivial if $x \neq 0_V$.
      There are called the coordinates of $x$ in $\{b_0, b_1, ..., b_{n-1}\}$.
    \end{theorem}

The uniqueness and the non-trivialness of the coordinates come from the fact that the basis contains linearly independent vectors.

.. raw:: latex

    \begin{theorem}[Vector subspace intersection]
      With $U_1$ and $U_2$ two vector subspaces of a $F$-vector space $V$, $U_1 \cap U_2$ is a vector subspace of $V$, $U_1$ and $U_2$.
    \end{theorem}

This is mainly due to the fact that :latex:`$0_V$` belongs to all these sets.

The following function is quite useful when working with vector spaces.

.. raw:: latex

    \begin{definition}[Kronecker delta function]
      \begin{eqnarray*}
        \delta: \N \times \N &\rightarrow& \{0, 1\} \\
        i, j &\mapsto& \delta_{i,j} =
          \left\{\begin{array}{l}
            0 \text{ if } i \neq j \\
            1 \text{ if } i = j
          \end{array}\right.
      \end{eqnarray*}
    \end{definition}


Dimension
---------

.. raw:: latex

    Given a $F$-vector space $V$ which has a finite generator set $x_0, x_1, ..., x_{n-1} \in V$, if this set is linearly dependent, it is possible to remove a vector from it in order to get a new set which also generated $V$.
    This operation can be repeated until the generator set is linerarly independent.
    The resulting set of vectors then defines a basis for $V$, by definition of what a basis is.

    Let $B = \{b_0, b_1, ..., b_{n-1}\}$ be the obtained basis.

    \begin{theorem}
      If $\{x_0, x_1, ..., x_{n-1}\}$ is another set of linearly independent vectors of $V$, it also generates $V$ and is therefore a basis of $V$.
    \end{theorem}

    Proof:
    As $B$ generates $V$, it is possible to write every $x_i$ as a linear combination of $b_j$.
    For $y \in V$, $y$ is also a linear combination of $b_j$.
    Let's name $\lambda_{i,j}$ and $\mu_j$ all the coefficients:
    \begin{eqnarray*}
      \forall i \in \llbracket 0, n - 1 \rrbracket, x_i &=& \sum_{j=0}^{n-1} \lambda_{i,j} b_j \\
      y &=& \sum_{j=0}^{n-1} \mu_j b_j
    \end{eqnarray*}

    $(\lambda_{i,j})$ is a square matrix. Let's try computing its inverse $(\alpha_{i,j})$ (this only works if the matrix is invertible), which is defined by:
    \begin{displaymath}
      \forall j, k \in \llbracket 0, n - 1 \rrbracket, \sum_{i=0}^{n-1} \alpha_{k,i} \lambda_{i,j} = \delta_{k,j}
    \end{displaymath}
    For each $k$, this expression can be seen as a system of $n$ equations (one for each $j$), which aims to express $b_k$ as a linear combination of $\{x_0, x_1, ..., x_{n-1}\}$.
    As there are $n$ equations with $n$ unknown variables, each system accepts at least one solution (the proof of $(\lambda_{i,j})$ being invertible is a bit more complex and uses the fact that the $\{x_i\}$ are linearly independent).
    Using this $(\alpha_{i,j})$ matrix:
    \begin{eqnarray*}
      \forall k \in \llbracket 0, n - 1 \rrbracket, \sum_{j=0}^{n-1} \sum_{i=0}^{n-1} \alpha_{k,i} \lambda_{i,j} b_j &=& \sum_{j=0}^{n-1} \delta_{k,j} b_j \\
      \forall k \in \llbracket 0, n - 1 \rrbracket, \sum_{i=0}^{n-1} \alpha_{k,i} x_i &=& b_k \\
      y = \sum_{k=0}^{n-1} \mu_k b_k &=& \sum_{k=0}^{n-1} \sum_{i=0}^{n-1} \mu_k \alpha_{k,i} x_i \\
      y &=& \sum_{i=0}^{n-1} \left(\sum_{k=0}^{n-1} \mu_k \alpha_{k,i}\right) . x_i
    \end{eqnarray*}

QED.

.. raw:: latex

    Therefore, if $\{y_0, y_1, ..., y_{m-1}\}$ is another basis of $V$:
    \begin{itemize}
      \item If $m > n$, $\{y_0, y_1, ..., y_{n-1}\}$ is a set of linearly independent vectors of $V$, so is a basis too, and $y_{m-1}$ can be written as a non-trivial linear combination of $y_0, y_1, ..., y_{n-1}$ (because $y_{m-1} != 0$). Doing so breaks the fact that $\{y_0, y_1, ..., y_{m-1}\}$ is a set of linearly independent vectors.
      \item If $m < n$, it is possible to follow the same reasoning with $\{b_0, ..., b_{m-1}\}$.
    \end{itemize}
    So $m = n$. This proves that every basis of $V$ shares the same size.

    \begin{definition}[Dimension]
      Given a $F$-vector space $V$ which has a finite generator set, it is possible to find a finite set of vectors which is a basis for $V$.
      In such a case, every basis of $V$ shares the same number of vectors, which is called the dimension of $V$, $\dim(V)$.
    \end{definition}

It is possible to build a basis from a set of generators, by eliminating vectors until obtaining a set of linerarly independent ones.
It is also possible to build a basis from a set of linerarly independent vectors, by adding more linerarly independent vectors until the set generates the whole vector space.
In both cases, the size of the resulting set of vectors will be the dimension of the vector space.

.. raw:: latex

    \begin{definition}[Infinite dimension]
      If there exists an infinite set of vectors which is linerarly independent, it is not possible to find a finite set of vectors which would be a basis.
      In this case, the vector space is said to have an infinite dimension.
    \end{definition}

    \begin{theorem}[Null dimension]
      If $V$ if of finite dimension,
      \begin{eqnarray*}
        \dim(V) = 0 &\Leftrightarrow& V = \{0_V\}
      \end{eqnarray*}
    \end{theorem}

    \begin{theorem}[Dimension of vector subspace intersection]
      With $U_1$ and $U_2$ two vector subspaces of a $F$-vector space $V$, if $U_1$ (resp. $U_2$) have a finite dimension, so does $U_1 \cap U_2$ and $\dim(U_1 \cap U_2) \le \dim(U_1)$ (resp. $\dim(U_1 \cap U_2) \le \dim(U_2)$).
    \end{theorem}


Canonical vector spaces
-----------------------

.. raw:: latex

    \begin{theorem}[Canonical vector spaces]
      Given a field $F$,
      \begin{itemize}
        \item $F^\N$ is a $F$-vector space of infinite dimension.
        \item For all $n \in \Ns$, $F^n$ is a $F$-vector space of dimension $n$.
      \end{itemize}
    \end{theorem}

    \begin{theorem}[Canonical base of $F^\N$]
      Let $e_i = (\delta_{i,j})_{j \in \N} \in F^\N$.
      $(e_i)_{i \in \N}$ is a base of the $F$-vector space $F^\N$.
    \end{theorem}

    \begin{theorem}[Canonical base of $F^n$]
      For $n \in \Ns$, let $e_i = (\delta_{i,j})_{j \in \llbracket 0, n - 1\rrbracket} \in F^n$.
      $(e_i)_{i \in \llbracket 0, n - 1\rrbracket}$ is a base of the $F$-vector space $F^n$.
    \end{theorem}

Linear map
----------

.. raw:: latex

    \begin{definition}[Linear map]
      A linear map $f$ between two $F$-vector spaces $V$ and $V'$ is a function from $V$ to $V'$ such that:
      \begin{displaymath}
        \forall \lambda \in F, \forall x, y \in V, f(\lambda . x + y) = \lambda . f(x) + f(y)
      \end{displaymath}
    \end{definition}

    \begin{definition}[Kernel]
      The kernel of the linear map $f$ is the set of vectors of $V$ which maps to $0_{V'}$.
      \begin{displaymath}
        \Ker(f) = \{x \in V: f(x) = 0_{V'}\}
      \end{displaymath}
    \end{definition}

    Properties:
    \begin{itemize}
      \item $f(0_V) = 0_{V'}$ and $0_V \in \Ker(f)$ and $0_{V'} \in \Img(f)$
      \item $\Ker(f)$ is a sub-vector space of $V$
      \item $\Img(f)$ defined as $f(V)$ is a sub-vector space of $V'$
      \item If $\{g_0, ..., g_{n-1}\}$ is a set of vectors that generates $V$, $\{f(g_0), ..., f(g_{n-1})\}$ generates $\Img(f)$
      \item If $V$ is of finite dimension, $\Img(f)$ too and $\dim(\Img(f)) \le \dim(V)$
      \item If $V$ is of finite dimension and $f$ is injective, $\dim(\Img(f)) = \dim(V)$
    \end{itemize}

    \begin{theorem}[Injective linear map]
      A linear map $f$ of a vector space $V$ is injective iff $\Ker(f) = \{0_V\}$
    \end{theorem}
    This comes from the fact that $f(x) - f(y) = f(x - y)$

    \begin{definition}[Endomorphism]
      An endomorphism of a vector space $V$ is a linear map $V \rightarrow V$.
    \end{definition}

    \begin{definition}[Isomorphism]
      An isomorphism between vector space $V$ and $V'$ is a bijective linear map $V \rightarrow V'$.
    \end{definition}

    \begin{definition}[Automorphism]
      An automorphism of a vector space $V$ is a bijective linear map $V \rightarrow V$.
    \end{definition}

    \begin{theorem}[Isomorphism reciprocal]
      The reciprocal of an isomorphism $f: V \rightarrow V'$ is an isomorphism $f^{-1}: V' \rightarrow V$.
    \end{theorem}

    \begin{theorem}[Isomorphism dimension]
      If $f: V \rightarrow V'$ is an isomorphism, $V$ and $V'$ share the same dimension.
    \end{theorem}

    \begin{theorem}[Basis representation uniqueness]
      If $B = (b_i)$ is a basis of $V$, the values of a linear map $f: V \rightarrow V'$ applied to $B$ define in a unique way this linear map.
      This linear map can be constructed applying linear combinations of $(f(b_i))$.
    \end{theorem}

    \begin{theorem}[Basis representation of finite dimension vector spaces]
      If $B = (b_i)$ is a basis of $V$ and $B' = (b'_i)$ one of $V'$, the coordinates of $f(b_i)$ over $B'$ define a matrix of $\dim(V)$ lines over $\dim(V')$ columns.
      This matrix defines $f$ in a unique way, according to the chosen bases $B$ and $B'$.
    \end{theorem}


Vector space sum
----------------

.. raw:: latex

   \begin{definition}[Vector space sum]
      With $U_1$ and $U_2$ two vector subspaces of a $F$-vector space $V$, $U_1 + U_2$ is the $F$-vector space consisting of the sums of the vectors from both subspaces:
      \begin{displaymath}
        U_1 + U_2 = \{u_1 + u_2, (u_1, u_2) \in U_1 \times U_2\}
      \end{displaymath}
    \end{definition}

    Properties:
    \begin{itemize}
      \item $U_1 + U_2$ is a vector subspace of $V$.
      \item $U_1 \subseteq U_1 + U_2$ (using $0_V \in U_2$) and $U_2 \subseteq U_1 + U_2$
      \item If $U_1$ and $U_2$ are of finite dimension, $U_1 + U_2$ too and $\dim(U_1 + U_2) \le \dim(U_1) + \dim(U_2)$.
      \item If $U_1$ and $U_2$ are of finite dimension and $U_1 \cap U_2 = \{0\}$, $U_1 + U_2$ is of finite dimension and $\dim(U_1 + U_2) = \dim(U_1) + \dim(U_2)$.
    \end{itemize}

    \begin{theorem}[Sum of dimensions of a linear map]
      If $V$ is a $F$-vector space of finite dimension and $f$ is a linear map from $V$,
      \begin{displaymath}
        \dim(\Ker(f)) + \dim(\Img(f)) = \dim(V)
      \end{displaymath}
    \end{theorem}
    Proof: let us build a basis of $V$ from a basis of $\Ker(f)$.
    With $(b_0, b_1, ..., b_{m-1})$ a basis of $\Ker(f)$ (which is of finite dimension), if $\Ker(f) \ne V$, choose $b_m \in V \backslash \Ker(f)$.
    The linear independent set of vectors $(b_i)$ can be expended by choosing $b_i \in V \backslash \text{span}(b_0, b_1, ..., b_{i-1})$, until this set is empty.
    Then, $\text{span}(b_0, b_1, ..., b_{i-1}) = V$ so $(b_0, b_1, ..., b_{i-1})$ is a basis of $V$.
    Let $U = \text{span}(b_m, b_{m+1}, ..., b_{i-1})$.
    With the basis that has been built, $\dim(\Ker(f)) + \dim(U) = \dim(V)$.
    $f_{|U}$ is a linear map from $U$ to $\Img(f)$ which kernel is:
    \begin{displaymath}
      Ker(f_{|U}) = \Ker(f) \cap U = \{0_V\}
    \end{displaymath}
    So $f_{|U}$ is injective. It is moreover surjective, using $V = \Ker(f) + U$, because this means that:
    \begin{displaymath}
      \Img(f) = f(V) = f(\Ker(f) + U) = f(\Ker(f)) + f(U) = {0_V} + \Img(f_{|U}) = \Img(f_{|U})
    \end{displaymath}
    Therefore $f_{|U}$ is an isomorphism from $U$ to $\Img(f)$, so $\dim(U) = \dim(\Img(f))$.

QED.

.. raw:: latex

    \begin{definition}[Vector space direct sum]
      With $U_1$ and $U_2$ two vector subspaces of a $F$-vector space $V$, $U_1 + U_2$ is a direct sum when every vector of the result can be written as $u_1 + u_2$ in a unique way.
      In this case, the sum is written $U_1 \oplus U_2$.
    \end{definition}

    \begin{theorem}[Vector space direct sum]
      $U_1 + U_2$ is a direct sum iff $U_1 \cap U_2 = \{0_V\}$
    \end{theorem}
    Proof: $U_1 + U_2$ is a direct sum iff $0_V$ can be written as $u_1 + u_2$ in a unique way (using vector addition relationships).
    This is equivalent to finding $u_1 \in U_1$ and $u_2 \in U_2$ such that $u_1 = -u_2$.
    Using $u_1 = u_2 = 0_V$ always works, and the non-uniqueness is equivalent to finding a non-null vector in $U_1 \cap U_2$.

QED.

.. raw:: latex

   This leads to this property: if $U_1$ and $U_2$ are of finite dimension and $U_1 \cap U_2 = \{0\}$, $U_1 \oplus U_2$ exists, is of finite dimension and $\dim(U_1 \oplus U_2) = \dim(U_1) + \dim(U_2)$.

In practise the direct sum can be used to break down a vector space into vector subspaces of interest.
